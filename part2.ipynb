{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling x \n",
    "x1 = np.random.normal(3 , 2, (1000000,) ) \n",
    "x2 = np.random.normal(-1, 2 , (1000000,)) \n",
    "x = np.zeros((1000000, 3)) \n",
    "\n",
    "x[:, 0] = 1 # adding intercept term \n",
    "x[:, 1] = x1 \n",
    "x[:, 2] = x2 \n",
    "# print(np.var(x[:, 1]))  \n",
    "\n",
    "# sampling y \n",
    "\n",
    "original_hypothesis = np.array([3,1,2]) \n",
    "y = np.matmul(x, original_hypothesis) \n",
    "y += np.random.normal(0, math.sqrt(2) , y.shape)  \n",
    "# print(np.mean(y)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_error(X, Y, theta, batch_size, batch_number):\n",
    "    X_batch = X[batch_size  * (batch_number - 1)   : batch_size * (batch_number - 1) + batch_size, :] \n",
    "    Y_batch = Y[ batch_size  * (batch_number - 1)   : batch_size * (batch_number - 1) + batch_size] \n",
    "\n",
    "    Z = Y_batch - np.matmul(X_batch, theta)\n",
    "    error = np.matmul(np.transpose(Z), Z) / (2 * batch_size) \n",
    "    return error  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_gradient(X, Y, theta, batch_size, batch_number):\n",
    "    X_batch = X[batch_size  * (batch_number - 1)   : batch_size * (batch_number - 1) + batch_size, :] \n",
    "    Y_batch = Y[ batch_size  * (batch_number - 1)   : batch_size * (batch_number - 1) + batch_size] \n",
    "    Z_batch = Y_batch - np.matmul(X_batch, theta) \n",
    "    # print(Z)\n",
    "    gradient = np.zeros(theta.size) \n",
    "    for j in range(theta.size):\n",
    "        X_j = X_batch[:, j] \n",
    "        gradient[j] = np.sum(Z_batch * X_j)/ batch_size \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(X, Y, batch_size, k, gamma):\n",
    "    current_batch_number = 1 \n",
    "    theta = np.zeros(X[0].size) \n",
    "    # print(theta) \n",
    "    initial_error = compute_batch_error(X, Y, theta,  batch_size, int(current_batch_number) ) \n",
    "    epsilon = 0.000001 * initial_error \n",
    "\n",
    "    # print(f\"factor is {pow(batch_size, -0.5)}\")\n",
    "    #we decrease epsilon for large batch sizes, since smaller batch sizes converge more noisily to the \n",
    "    # optima, thus they require a larger epsilon \n",
    "     \n",
    "    epsilon = pow(batch_size, -0.5) * epsilon \n",
    "    max_count = int(1/epsilon) \n",
    "    # to avoid oscillations, we introduce this cap  \n",
    "\n",
    "\n",
    "    learning_parameter = 0.001\n",
    "    MOD = Y.size / batch_size \n",
    "    max_count = max(max_count, MOD)  \n",
    "    print(f\"maxcount is {max_count}\")\n",
    "    no_of_iterations = 0 \n",
    "\n",
    "\n",
    "    error_arr = []\n",
    "    running_avg = 0 \n",
    "    previous_running_avg = 0 \n",
    "\n",
    "    for i in range(k):\n",
    "        grad = compute_batch_gradient(X, Y, theta, batch_size, int(current_batch_number) ) \n",
    "        theta = theta + learning_parameter * grad \n",
    "        curr_error = compute_batch_error(X, Y,theta,  batch_size, int(current_batch_number) )\n",
    "        error_arr.append(curr_error) \n",
    "        running_avg += curr_error \n",
    "        current_batch_number = (current_batch_number + 1) \n",
    "        if (current_batch_number > MOD):  current_batch_number -= MOD \n",
    "        # no_of_iterations += 1\n",
    "\n",
    "\n",
    "    running_avg /= k\n",
    "    last_count = 0  # no of consecutive times difference in averages is less than epsilon  \n",
    "    # epsilon = 0.000001 * running_avg \n",
    "    \n",
    "    while (last_count < gamma and no_of_iterations < max_count):\n",
    "        \n",
    "        \n",
    "        previous_running_avg = running_avg \n",
    "        previous_error = error_arr.pop(0) \n",
    "\n",
    "        # print(f\" batch size, batch num {batch_size} {current_batch_number}\") \n",
    "        grad = compute_batch_gradient(X, Y, theta, batch_size, int(current_batch_number) ) \n",
    "        theta = theta + learning_parameter * grad \n",
    "        \n",
    "        curr_error = compute_batch_error(X, Y,theta,  batch_size, int(current_batch_number) ) \n",
    "        error_arr.append(curr_error) \n",
    "\n",
    "        running_avg = running_avg + (curr_error - previous_error)/k  \n",
    "\n",
    "        # print(f\"current theta is {theta}\") \n",
    "        # print(f\"last count {last_count}\")\n",
    "        # print(f\"average error is {running_avg}\") \n",
    "        \n",
    "        if (abs(previous_running_avg - running_avg) < epsilon) :  last_count += 1 \n",
    "        else: last_count = 0\n",
    "        \n",
    "        no_of_iterations += 1 \n",
    "        current_batch_number = (current_batch_number + 1) \n",
    "        if (current_batch_number > MOD):  current_batch_number -= MOD \n",
    "\n",
    "    print(f\"no of iterations is {no_of_iterations}\") \n",
    "    # print(f\"learned theta is {theta}\") \n",
    "    return theta \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, Y):\n",
    "    theta = np.zeros(X[0].size) \n",
    "    print(theta) \n",
    "    initial_error = compute_batch_error(X, Y, theta,  Y.size , 1 ) \n",
    "    error = initial_error \n",
    "    epsilon = 0.0001 * initial_error \n",
    "    # print(f\"intial error is {initial_error}\")\n",
    "    learning_parameter = 0.001\n",
    "    no_of_iterations = 0 \n",
    "\n",
    "    previous_error = 2*error \n",
    "    while ( previous_error - error > epsilon) : \n",
    "\n",
    "        previous_error = error \n",
    "        grad = compute_batch_gradient(X, Y, theta, Y.size, 1 ) \n",
    "        theta = theta + learning_parameter * grad \n",
    "        \n",
    "        error = compute_batch_error(X, Y,theta,  Y.size , 1 ) \n",
    "\n",
    "\n",
    "        # print(f\"current theta is {theta}\") \n",
    "\n",
    "        no_of_iterations += 1 \n",
    "\n",
    "    print(f\"no of iterations is {no_of_iterations}\") \n",
    "    print(f\"learned theta is {theta}\") \n",
    "    return theta \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(X, Y, hypothesis):\n",
    "    Z = (Y - np.matmul(X, hypothesis)) \n",
    "    m = Y.size \n",
    "    error = np.matmul(np.transpose(Z), Z)/ (2 * m) \n",
    "    return error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxcount is 1000000.0\n",
      "no of iterations is 90164\n",
      "model1 error1 [3.01095502 0.97275792 1.96257408] 1.097125074271597\n",
      "maxcount is 516992\n",
      "no of iterations is 2400\n",
      "maxcount is 5211175\n",
      "no of iterations is 4522\n",
      "maxcount is 52476196\n",
      "no of iterations is 19078\n"
     ]
    }
   ],
   "source": [
    "test_set = pd.read_csv(\"./ass1_data/data/q2/q2test.csv\") \n",
    "test_set = test_set.to_numpy() \n",
    "temp = np.zeros((10000, 4)) \n",
    "temp[:,0] = 1 \n",
    "temp[:, 1] = test_set[:, 0] \n",
    "temp[:, 2] = test_set[:, 1] \n",
    "temp[:, 3] = test_set[:, 2]\n",
    "\n",
    "test_set = temp \n",
    "\n",
    "# print(test_set) \n",
    "\n",
    "test_input = test_set[:, 0 : 3] \n",
    "test_output = test_set[:, 3] \n",
    "\n",
    "model1 = minibatch_gradient_descent(x , y , 1, 10,2) \n",
    "error1 = compute_error(test_input, test_output, model1) \n",
    "print(\"model1 error1\", model1, error1 )\n",
    "model2 = minibatch_gradient_descent(x, y, 100, 10,1)  \n",
    "model3 = minibatch_gradient_descent(x, y , 10000, 5,1) \n",
    "model4 = minibatch_gradient_descent(x, y, 1000000, 1,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.01095502 0.97275792 1.96257408]\n",
      "[1.5520758  1.31782229 1.89540753]\n",
      "[2.18568377 1.17847891 1.94092125]\n",
      "[2.98348266 1.00425365 1.99914597]\n",
      "error wrt learned models\n",
      "1.097125074271597\n",
      "7.067409341304615\n",
      "2.9051798719931075\n",
      "0.9839013787157844\n",
      "error wrt original model\n",
      "0.9829469215\n"
     ]
    }
   ],
   "source": [
    "error1 = compute_error(test_input, test_output, model1) \n",
    "error2 = compute_error(test_input, test_output, model2) \n",
    "error3 = compute_error(test_input, test_output, model3) \n",
    "error4 = compute_error(test_input, test_output, model4) \n",
    "\n",
    "print(model1) \n",
    "print(model2) \n",
    "print(model3)\n",
    "print(model4)\n",
    "\n",
    "print(\"error wrt learned models\") \n",
    "print(error1) \n",
    "print(error2)\n",
    "print(error3) \n",
    "print(error4)\n",
    "\n",
    "og_error1 = compute_error(test_input, test_output, original_hypothesis) \n",
    "\n",
    "print(\"error wrt original model\") \n",
    "print(og_error1) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
